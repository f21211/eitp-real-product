#!/usr/bin/env python3
"""
æ•™ç¨‹1ï¼šä½¿ç”¨EIT-Pè®­ç»ƒæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å…¥é—¨æ•™ç¨‹ï¼Œå±•ç¤ºå¦‚ä½•ï¼š
1. åŠ è½½æ¨¡å‹
2. å‡†å¤‡æ•°æ®
3. ä½¿ç”¨EIT-Pè®­ç»ƒ
4. ç›‘æ§CEPå‚æ•°
5. è¯„ä¼°ç»“æœ

é¢„è®¡æ—¶é—´ï¼š30-60åˆ†é’Ÿ
éš¾åº¦ï¼šå…¥é—¨
"""

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import time
from datetime import datetime

print("=" * 80)
print("ğŸ“ æ•™ç¨‹1ï¼šä½¿ç”¨EIT-Pè®­ç»ƒæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹")
print("=" * 80)
print()

# ============================================================================
# æ­¥éª¤1ï¼šç¯å¢ƒæ£€æŸ¥
# ============================================================================

print("æ­¥éª¤1ï¼šæ£€æŸ¥ç¯å¢ƒ")
print("-" * 80)

print(f"Python version: {torch.__version__}")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

print()

# ============================================================================
# æ­¥éª¤2ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# ============================================================================

print("æ­¥éª¤2ï¼šåŠ è½½GPT-2æ¨¡å‹")
print("-" * 80)

print("åŠ è½½tokenizer...")
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

print("åŠ è½½æ¨¡å‹...")
model = GPT2LMHeadModel.from_pretrained('gpt2')

if torch.cuda.is_available():
    model = model.cuda()
    print("âœ… æ¨¡å‹å·²ç§»åˆ°GPU")

print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
print()

# ============================================================================
# æ­¥éª¤3ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
# ============================================================================

print("æ­¥éª¤3ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®")
print("-" * 80)

# ç®€å•çš„è®­ç»ƒæ ·æœ¬
train_texts = [
    "The quick brown fox jumps over the lazy dog.",
    "Machine learning is transforming artificial intelligence.",
    "Physics provides the fundamental laws of nature.",
    "Consciousness emerges from complex systems.",
    "Intelligence requires energy and complexity.",
    "The universe follows mathematical principles.",
    "Quantum mechanics describes the microscopic world.",
    "Neural networks learn from data patterns.",
]

print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_texts)}")

# Tokenize
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=32,
    return_tensors='pt'
)

if torch.cuda.is_available():
    train_encodings = {k: v.cuda() for k, v in train_encodings.items()}

print(f"Tokenæ•°é‡: {train_encodings['input_ids'].shape}")
print()

# ============================================================================
# æ­¥éª¤4ï¼šå®šä¹‰CEPç›‘æ§å‡½æ•°
# ============================================================================

print("æ­¥éª¤4ï¼šå®šä¹‰CEPå‚æ•°ç›‘æ§")
print("-" * 80)

def calculate_fractal_dimension(tensor):
    """ç®€åŒ–çš„åˆ†å½¢ç»´åº¦è®¡ç®—"""
    if tensor.numel() == 0:
        return 2.0
    
    # ä½¿ç”¨å¼ é‡çš„ç»Ÿè®¡ç‰¹æ€§ä¼°è®¡åˆ†å½¢ç»´åº¦
    flat = tensor.view(-1).cpu().detach().numpy()
    import numpy as np
    
    # ç®€åŒ–æ–¹æ³•ï¼šä½¿ç”¨æ ‡å‡†å·®å’ŒèŒƒå›´çš„æ¯”ç‡
    std = np.std(flat)
    range_val = np.ptp(flat)  # peak-to-peak
    
    if range_val > 0:
        D = 2.0 + np.log(std / (range_val + 1e-8))
        return max(1.5, min(3.5, abs(D)))  # é™åˆ¶åœ¨åˆç†èŒƒå›´
    return 2.0

def calculate_complexity_coefficient(model):
    """è®¡ç®—å¤æ‚åº¦ç³»æ•°Î»"""
    total = 0
    active = 0
    
    for param in model.parameters():
        total += param.numel()
        active += (param.abs() > 1e-3).sum().item()
    
    return active / total if total > 0 else 0.0

def calculate_iem_energy(model, alpha=1.0):
    """è®¡ç®—IEMèƒ½é‡"""
    # H - ä¿¡æ¯ç†µï¼ˆç”¨å‚æ•°åˆ†å¸ƒçš„ç†µè¿‘ä¼¼ï¼‰
    H = 0.0
    for param in model.parameters():
        if param.numel() > 0:
            p = torch.softmax(param.view(-1), dim=0)
            H += -(p * torch.log(p + 1e-10)).sum().item()
    
    # T - æ¸©åº¦ï¼ˆç”¨æ¢¯åº¦çš„æ ‡å‡†å·®è¿‘ä¼¼ï¼‰
    T = 1.0  # ç®€åŒ–ä¸ºå¸¸æ•°
    
    # C - è¿è´¯æ€§ï¼ˆç”¨å‚æ•°çš„ç›¸å…³æ€§è¿‘ä¼¼ï¼‰
    C = 0.9  # ç®€åŒ–ä¸ºå¸¸æ•°
    
    IEM = alpha * H * T * C
    return IEM

print("âœ… CEPç›‘æ§å‡½æ•°å·²å®šä¹‰")
print()

# ============================================================================
# æ­¥éª¤5ï¼šè®­ç»ƒå¾ªç¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================================

print("æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ")
print("-" * 80)

# è®­ç»ƒå‚æ•°
epochs = 3
learning_rate = 5e-5
alpha = 1.0  # IEMç³»æ•°

# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# è®°å½•
history = {
    'loss': [],
    'fractal_dimension': [],
    'complexity_coefficient': [],
    'iem_energy': [],
    'time': []
}

print(f"è®­ç»ƒé…ç½®:")
print(f"  Epochs: {epochs}")
print(f"  Learning rate: {learning_rate}")
print(f"  Alpha (IEM): {alpha}")
print()

model.train()
start_time = time.time()

for epoch in range(epochs):
    epoch_start = time.time()
    total_loss = 0
    
    # ç®€å•çš„è®­ç»ƒå¾ªç¯
    for i in range(0, len(train_texts), 2):  # Batch size = 2
        # å‡†å¤‡batch
        batch_size = min(2, len(train_texts) - i)
        batch_texts = train_texts[i:i+batch_size]
        
        batch_enc = tokenizer(
            batch_texts,
            truncation=True,
            padding=True,
            max_length=32,
            return_tensors='pt'
        )
        
        if torch.cuda.is_available():
            batch_enc = {k: v.cuda() for k, v in batch_enc.items()}
        
        # Forward pass
        outputs = model(**batch_enc, labels=batch_enc['input_ids'])
        loss = outputs.loss
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / (len(train_texts) // 2)
    
    # è®¡ç®—CEPå‚æ•°
    with torch.no_grad():
        # è·å–æ¨¡å‹è¾“å‡ºç”¨äºè®¡ç®—åˆ†å½¢ç»´åº¦
        sample_output = model(**train_encodings, labels=train_encodings['input_ids'])
        logits = sample_output.logits
        
        D = calculate_fractal_dimension(logits)
        lambda_val = calculate_complexity_coefficient(model)
        iem = calculate_iem_energy(model, alpha)
    
    epoch_time = time.time() - epoch_start
    
    # è®°å½•
    history['loss'].append(avg_loss)
    history['fractal_dimension'].append(D)
    history['complexity_coefficient'].append(lambda_val)
    history['iem_energy'].append(iem)
    history['time'].append(epoch_time)
    
    # æ˜¾ç¤ºè¿›åº¦
    print(f"Epoch {epoch+1}/{epochs}:")
    print(f"  Loss: {avg_loss:.4f}")
    print(f"  åˆ†å½¢ç»´åº¦ D: {D:.3f} {'âœ…' if D >= 2.7 else 'â³'} (ç›®æ ‡: â‰¥2.7)")
    print(f"  å¤æ‚åº¦ç³»æ•° Î»: {lambda_val:.3f} {'âœ…' if lambda_val >= 0.8 else 'â³'} (ç›®æ ‡: â‰¥0.8)")
    print(f"  IEMèƒ½é‡: {iem:.6f}")
    print(f"  æ—¶é—´: {epoch_time:.2f}s")
    
    # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æ¶Œç°é˜ˆå€¼
    if D >= 2.7 and lambda_val >= 0.8:
        print("  ğŸ‰ è¾¾åˆ°æ™ºèƒ½æ¶Œç°é˜ˆå€¼ï¼")
    
    print()

total_time = time.time() - start_time
print(f"è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time:.2f}s")
print()

# ============================================================================
# æ­¥éª¤6ï¼šè¯„ä¼°å’Œæµ‹è¯•
# ============================================================================

print("æ­¥éª¤6ï¼šè¯„ä¼°æ¨¡å‹")
print("-" * 80)

model.eval()

# æµ‹è¯•ç”Ÿæˆ
test_prompts = [
    "The future of AI is",
    "Intelligence emerges from",
    "Physics and AI are",
]

print("ç”Ÿæˆæµ‹è¯•:")
for prompt in test_prompts:
    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']
    if torch.cuda.is_available():
        input_ids = input_ids.cuda()
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=20,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True
        )
    
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"  è¾“å…¥: {prompt}")
    print(f"  è¾“å‡º: {generated_text}")
    print()

# ============================================================================
# æ­¥éª¤7ï¼šç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
# ============================================================================

print("æ­¥éª¤7ï¼šç”Ÿæˆè®­ç»ƒæŠ¥å‘Š")
print("-" * 80)

report = f"""
# è®­ç»ƒæŠ¥å‘Š

**è®­ç»ƒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ€»æ—¶é•¿**: {total_time:.2f}ç§’

## é…ç½®

- æ¨¡å‹: GPT-2 Small
- è®­ç»ƒæ ·æœ¬: {len(train_texts)}
- Epochs: {epochs}
- Learning rate: {learning_rate}
- Alpha (IEM): {alpha}

## è®­ç»ƒç»“æœ

### Lossæ›²çº¿

| Epoch | Loss | æ—¶é—´(s) |
|-------|------|---------|
"""

for i in range(epochs):
    report += f"| {i+1} | {history['loss'][i]:.4f} | {history['time'][i]:.2f} |\n"

report += f"""

### CEPå‚æ•°æ¼”åŒ–

| Epoch | D | Î» | IEMèƒ½é‡ | è¾¾åˆ°é˜ˆå€¼ |
|-------|---|---|---------|----------|
"""

for i in range(epochs):
    meets = "âœ…" if history['fractal_dimension'][i] >= 2.7 and history['complexity_coefficient'][i] >= 0.8 else "âŒ"
    report += f"| {i+1} | {history['fractal_dimension'][i]:.3f} | {history['complexity_coefficient'][i]:.3f} | {history['iem_energy'][i]:.6f} | {meets} |\n"

report += f"""

## åˆ†æ

**æœ€ç»ˆCEPå‚æ•°**:
- åˆ†å½¢ç»´åº¦ D: {history['fractal_dimension'][-1]:.3f}
- å¤æ‚åº¦ç³»æ•° Î»: {history['complexity_coefficient'][-1]:.3f}
- IEMèƒ½é‡: {history['iem_energy'][-1]:.6f}

**æ˜¯å¦è¾¾åˆ°æ™ºèƒ½æ¶Œç°é˜ˆå€¼**:
- D â‰¥ 2.7: {'âœ… æ˜¯' if history['fractal_dimension'][-1] >= 2.7 else 'âŒ å¦'}
- Î» â‰¥ 0.8: {'âœ… æ˜¯' if history['complexity_coefficient'][-1] >= 0.8 else 'âŒ å¦'}

## ç»“è®º

è¿™æ˜¯ä¸€ä¸ª{'æˆåŠŸçš„' if history['fractal_dimension'][-1] >= 2.7 and history['complexity_coefficient'][-1] >= 0.8 else 'åˆæ­¥çš„'}è®­ç»ƒå®éªŒã€‚
{'æ¨¡å‹å·²è¾¾åˆ°æ™ºèƒ½æ¶Œç°çš„CEPé˜ˆå€¼æ¡ä»¶ã€‚' if history['fractal_dimension'][-1] >= 2.7 and history['complexity_coefficient'][-1] >= 0.8 else 'éœ€è¦æ›´å¤šè®­ç»ƒæˆ–è°ƒæ•´å‚æ•°ä»¥è¾¾åˆ°æ¶Œç°é˜ˆå€¼ã€‚'}

## ä¸‹ä¸€æ­¥

1. å°è¯•æ›´å¤šè®­ç»ƒæ•°æ®
2. è°ƒæ•´alphaå‚æ•°
3. å¢åŠ è®­ç»ƒepochs
4. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}
"""

# ä¿å­˜æŠ¥å‘Š
report_file = 'tutorial_01_training_report.md'
with open(report_file, 'w', encoding='utf-8') as f:
    f.write(report)

print(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
print()

# ============================================================================
# æ€»ç»“
# ============================================================================

print("=" * 80)
print("ğŸŠ æ•™ç¨‹1å®Œæˆï¼")
print("=" * 80)
print()
print("æ‚¨å·²ç»å­¦ä¼šäº†:")
print("  âœ… åŠ è½½å’Œé…ç½®EIT-Pç¯å¢ƒ")
print("  âœ… è®­ç»ƒä¸€ä¸ªç®€å•æ¨¡å‹")
print("  âœ… ç›‘æ§CEPå‚æ•°")
print("  âœ… ç†è§£æ™ºèƒ½æ¶Œç°é˜ˆå€¼")
print()
print("ä¸‹ä¸€æ­¥:")
print("  1. æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Š: cat tutorial_01_training_report.md")
print("  2. å°è¯•ä¿®æ”¹å‚æ•°é‡æ–°è®­ç»ƒ")
print("  3. è¿›å…¥æ•™ç¨‹2: TUTORIAL_02_CONSCIOUSNESS_DETECTION.py")
print()
print("=" * 80)

