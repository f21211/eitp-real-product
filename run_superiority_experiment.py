#!/usr/bin/env python3
"""
EIT-Pä¼˜è¶Šæ€§éªŒè¯å®éªŒæ‰§è¡Œè„šæœ¬
å¯¹æ¯”EIT-Pä¸ä¼ ç»ŸLLMçš„æ€§èƒ½å·®å¼‚
"""

import os
import sys
import json
import time
import numpy as np
import torch
import torch.nn as nn
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/mnt/sda1/myproject/datainall/AGI')

from eit_p.utils import get_global_logger, ConfigManager
from eit_p.training.eitp_trainer import EITPTrainer
from eit_p.losses.total_loss import TotalLoss
from eit_p.regularization.path_norm import PathNormRegularizer
from eit_p.regularization.entropy import EntropyRegularizer
from eit_p.regularization.chaos import ChaosRegularizer

class SuperiorityExperiment:
    """EIT-Pä¼˜è¶Šæ€§éªŒè¯å®éªŒ"""
    
    def __init__(self, config_path='EITP_Superiority_Experiment_Design.json'):
        self.logger = get_global_logger()
        self.config_manager = ConfigManager()
        self.experiment_config = self.load_experiment_config(config_path)
        self.results = {
            'experiment_id': self.experiment_config['experiment_id'],
            'start_time': datetime.now().isoformat(),
            'control_group_results': [],
            'treatment_group_results': [],
            'comparative_analysis': {}
        }
        
    def load_experiment_config(self, config_path):
        """åŠ è½½å®éªŒé…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def setup_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        self.logger.info('ğŸ”§ è®¾ç½®å®éªŒç¯å¢ƒ...')
        
        # è®¾ç½®CUDAç¯å¢ƒ
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            self.logger.info(f'âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}')
        else:
            self.device = torch.device('cpu')
            self.logger.warning('âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU')
            
        return True
    
    def create_baseline_model(self, model_size='117M'):
        """åˆ›å»ºä¼ ç»ŸLLMåŸºçº¿æ¨¡å‹"""
        self.logger.info(f'ğŸ“Š åˆ›å»ºä¼ ç»ŸLLMåŸºçº¿æ¨¡å‹ ({model_size})...')
        
        # ç®€åŒ–çš„GPT-2æ¶æ„
        config = {
            'vocab_size': 50257,
            'n_positions': 1024,
            'n_ctx': 1024,
            'n_embd': 768 if model_size == '117M' else 1024,
            'n_layer': 12 if model_size == '117M' else 24,
            'n_head': 12 if model_size == '117M' else 16,
            'activation_function': 'gelu_new',
            'resid_pdrop': 0.1,
            'embd_pdrop': 0.1,
            'attn_pdrop': 0.1,
            'layer_norm_epsilon': 1e-5,
            'initializer_range': 0.02,
            'use_cache': True
        }
        
        return config
    
    def create_eitp_model(self, model_size='117M'):
        """åˆ›å»ºEIT-Pæ¨¡å‹"""
        self.logger.info(f'ğŸ§  åˆ›å»ºEIT-Pæ¨¡å‹ ({model_size})...')
        
        # EIT-På¢å¼ºé…ç½®
        config = {
            'vocab_size': 50257,
            'n_positions': 1024,
            'n_ctx': 1024,
            'n_embd': 768 if model_size == '117M' else 1024,
            'n_layer': 12 if model_size == '117M' else 24,
            'n_head': 12 if model_size == '117M' else 16,
            'activation_function': 'gelu_new',
            'resid_pdrop': 0.1,
            'embd_pdrop': 0.1,
            'attn_pdrop': 0.1,
            'layer_norm_epsilon': 1e-5,
            'initializer_range': 0.02,
            'use_cache': True,
            # EIT-Pç‰¹æœ‰é…ç½®
            'iem_enhanced': True,
            'thermodynamic_optimization': True,
            'chaos_control': True,
            'coherence_loss': True
        }
        
        return config
    
    def measure_memory_efficiency(self, model, data_loader, model_type='baseline'):
        """æµ‹é‡å†…å­˜æ•ˆç‡"""
        self.logger.info(f'ğŸ’¾ æµ‹é‡{model_type}å†…å­˜æ•ˆç‡...')
        
        # è®°å½•åˆå§‹å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            max_memory = torch.cuda.max_memory_allocated()
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            model.train()
            total_memory = 0
            memory_samples = []
            
            for i, batch in enumerate(data_loader):
                if i >= 10:  # åªæµ‹è¯•å‰10ä¸ªbatch
                    break
                    
                # å‰å‘ä¼ æ’­
                outputs = model(batch['input_ids'])
                
                # è®°å½•å†…å­˜ä½¿ç”¨
                current_memory = torch.cuda.memory_allocated()
                memory_samples.append(current_memory)
                total_memory += current_memory
                
                # æ¸…ç†å†…å­˜
                del outputs
                torch.cuda.empty_cache()
            
            # è®¡ç®—å†…å­˜æ•ˆç‡
            avg_memory = np.mean(memory_samples)
            memory_efficiency = (avg_memory / (24 * 1024**3)) * 100  # å‡è®¾24GB GPU
            
            return {
                'initial_memory': initial_memory,
                'max_memory': max_memory,
                'avg_memory': avg_memory,
                'memory_efficiency': memory_efficiency,
                'memory_samples': memory_samples
            }
        else:
            return {
                'initial_memory': 0,
                'max_memory': 0,
                'avg_memory': 0,
                'memory_efficiency': 0,
                'memory_samples': []
            }
    
    def measure_training_stability(self, model, data_loader, epochs=3):
        """æµ‹é‡è®­ç»ƒç¨³å®šæ€§"""
        self.logger.info('ğŸ“ˆ æµ‹é‡è®­ç»ƒç¨³å®šæ€§...')
        
        stability_metrics = {
            'memory_overflows': 0,
            'training_interruptions': 0,
            'loss_variance': [],
            'gradient_norms': [],
            'learning_rate_stability': []
        }
        
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            epoch_losses = []
            
            for i, batch in enumerate(data_loader):
                try:
                    # å‰å‘ä¼ æ’­
                    outputs = model(batch['input_ids'])
                    loss = criterion(outputs.logits, batch['labels'])
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    loss.backward()
                    
                    # è®°å½•æ¢¯åº¦èŒƒæ•°
                    total_norm = 0
                    for p in model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm = total_norm ** (1. / 2)
                    stability_metrics['gradient_norms'].append(total_norm)
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    
                    optimizer.step()
                    epoch_losses.append(loss.item())
                    
                except RuntimeError as e:
                    if 'out of memory' in str(e):
                        stability_metrics['memory_overflows'] += 1
                        torch.cuda.empty_cache()
                    else:
                        stability_metrics['training_interruptions'] += 1
                        self.logger.error(f'è®­ç»ƒä¸­æ–­: {e}')
                
                # æ¸…ç†å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # è®°å½•æŸå¤±æ–¹å·®
            if epoch_losses:
                stability_metrics['loss_variance'].append(np.var(epoch_losses))
        
        return stability_metrics
    
    def run_control_group_experiment(self, model_size='117M'):
        """è¿è¡Œå¯¹ç…§ç»„å®éªŒï¼ˆä¼ ç»ŸLLMï¼‰"""
        self.logger.info(f'ğŸ”¬ è¿è¡Œå¯¹ç…§ç»„å®éªŒ ({model_size})...')
        
        # åˆ›å»ºåŸºçº¿æ¨¡å‹
        model_config = self.create_baseline_model(model_size)
        
        # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
        data_loader = self.create_mock_data_loader()
        
        # æµ‹é‡æ€§èƒ½æŒ‡æ ‡
        start_time = time.time()
        
        # å†…å­˜æ•ˆç‡æµ‹è¯•
        memory_metrics = self.measure_memory_efficiency(None, data_loader, 'baseline')
        
        # è®­ç»ƒç¨³å®šæ€§æµ‹è¯•
        stability_metrics = self.measure_training_stability(None, data_loader)
        
        end_time = time.time()
        
        results = {
            'model_type': 'baseline',
            'model_size': model_size,
            'training_time': end_time - start_time,
            'memory_metrics': memory_metrics,
            'stability_metrics': stability_metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results['control_group_results'].append(results)
        return results
    
    def run_treatment_group_experiment(self, model_size='117M'):
        """è¿è¡Œå®éªŒç»„å®éªŒï¼ˆEIT-Pï¼‰"""
        self.logger.info(f'ğŸ§  è¿è¡Œå®éªŒç»„å®éªŒ ({model_size})...')
        
        # åˆ›å»ºEIT-Pæ¨¡å‹
        model_config = self.create_eitp_model(model_size)
        
        # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
        data_loader = self.create_mock_data_loader()
        
        # æµ‹é‡æ€§èƒ½æŒ‡æ ‡
        start_time = time.time()
        
        # å†…å­˜æ•ˆç‡æµ‹è¯•
        memory_metrics = self.measure_memory_efficiency(None, data_loader, 'eitp')
        
        # è®­ç»ƒç¨³å®šæ€§æµ‹è¯•
        stability_metrics = self.measure_training_stability(None, data_loader)
        
        end_time = time.time()
        
        results = {
            'model_type': 'eitp',
            'model_size': model_size,
            'training_time': end_time - start_time,
            'memory_metrics': memory_metrics,
            'stability_metrics': stability_metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        self.results['treatment_group_results'].append(results)
        return results
    
    def create_mock_data_loader(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨"""
        # æ¨¡æ‹Ÿæ•°æ®
        mock_data = []
        for i in range(100):
            mock_data.append({
                'input_ids': torch.randint(0, 50257, (32, 128)),
                'labels': torch.randint(0, 50257, (32, 128))
            })
        return mock_data
    
    def run_statistical_analysis(self):
        """è¿è¡Œç»Ÿè®¡åˆ†æ"""
        self.logger.info('ğŸ“Š è¿è¡Œç»Ÿè®¡åˆ†æ...')
        
        # æå–å…³é”®æŒ‡æ ‡
        control_memory = [r['memory_metrics']['memory_efficiency'] for r in self.results['control_group_results']]
        treatment_memory = [r['memory_metrics']['memory_efficiency'] for r in self.results['treatment_group_results']]
        
        control_stability = [r['stability_metrics']['memory_overflows'] for r in self.results['control_group_results']]
        treatment_stability = [r['stability_metrics']['memory_overflows'] for r in self.results['treatment_group_results']]
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        analysis = {
            'memory_efficiency': {
                'control_mean': np.mean(control_memory) if control_memory else 0,
                'treatment_mean': np.mean(treatment_memory) if treatment_memory else 0,
                'improvement': 0,
                'effect_size': 0
            },
            'training_stability': {
                'control_overflows': np.mean(control_stability) if control_stability else 0,
                'treatment_overflows': np.mean(treatment_stability) if treatment_stability else 0,
                'improvement': 0,
                'effect_size': 0
            }
        }
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        if control_memory and treatment_memory:
            analysis['memory_efficiency']['improvement'] = (
                (np.mean(treatment_memory) - np.mean(control_memory)) / np.mean(control_memory) * 100
            )
        
        if control_stability and treatment_stability:
            analysis['training_stability']['improvement'] = (
                (np.mean(control_stability) - np.mean(treatment_stability)) / np.mean(control_stability) * 100
            )
        
        self.results['comparative_analysis'] = analysis
        return analysis
    
    def generate_report(self):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        self.logger.info('ğŸ“‹ ç”Ÿæˆå®éªŒæŠ¥å‘Š...')
        
        self.results['end_time'] = datetime.now().isoformat()
        self.results['total_experiments'] = len(self.results['control_group_results']) + len(self.results['treatment_group_results'])
        
        # ä¿å­˜ç»“æœ
        with open('EITP_Superiority_Experiment_Results.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        print('\nğŸ¯ EIT-Pä¼˜è¶Šæ€§éªŒè¯å®éªŒæŠ¥å‘Š')
        print('=' * 60)
        print(f'å®éªŒID: {self.results["experiment_id"]}')
        print(f'å¼€å§‹æ—¶é—´: {self.results["start_time"]}')
        print(f'ç»“æŸæ—¶é—´: {self.results["end_time"]}')
        print(f'æ€»å®éªŒæ•°: {self.results["total_experiments"]}')
        print()
        
        if 'comparative_analysis' in self.results:
            analysis = self.results['comparative_analysis']
            print('ğŸ“Š å…³é”®å‘ç°:')
            print(f'  â€¢ å†…å­˜æ•ˆç‡æ”¹è¿›: {analysis["memory_efficiency"]["improvement"]:.2f}%')
            print(f'  â€¢ è®­ç»ƒç¨³å®šæ€§æ”¹è¿›: {analysis["training_stability"]["improvement"]:.2f}%')
            print()
        
        print('âœ… å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° EITP_Superiority_Experiment_Results.json')
        
        return self.results
    
    def run_full_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        self.logger.info('ğŸš€ å¼€å§‹EIT-Pä¼˜è¶Šæ€§éªŒè¯å®éªŒ...')
        
        # è®¾ç½®ç¯å¢ƒ
        self.setup_environment()
        
        # è¿è¡Œå¯¹ç…§ç»„å®éªŒ
        self.logger.info('ğŸ”¬ è¿è¡Œå¯¹ç…§ç»„å®éªŒ...')
        for model_size in ['117M', '345M']:
            self.run_control_group_experiment(model_size)
        
        # è¿è¡Œå®éªŒç»„å®éªŒ
        self.logger.info('ğŸ§  è¿è¡Œå®éªŒç»„å®éªŒ...')
        for model_size in ['117M', '345M']:
            self.run_treatment_group_experiment(model_size)
        
        # ç»Ÿè®¡åˆ†æ
        self.run_statistical_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        return self.generate_report()

if __name__ == '__main__':
    # è¿è¡Œå®éªŒ
    experiment = SuperiorityExperiment()
    results = experiment.run_full_experiment()
