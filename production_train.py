#!/usr/bin/env python3
"""
EIT-P ç”Ÿäº§çº§è®­ç»ƒè„šæœ¬
é›†æˆå®éªŒç®¡ç†ã€æ¨¡å‹æ³¨å†Œå’ŒæŒ‡æ ‡è·Ÿè¸ª
"""

import os
import sys
import time
import json
import torch
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments
from eit_p.training import EITPTrainer
from eit_p.experiments import ExperimentManager, ModelRegistry, MetricsTracker
from eit_p.experiments.experiment_manager import ExperimentConfig
from eit_p.experiments.model_registry import ModelMetadata
from eit_p.utils import get_global_logger, ConfigManager


class ProductionTrainer:
    """ç”Ÿäº§çº§è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config_manager = ConfigManager(config_path)
        self.logger = get_global_logger()
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        self.experiment_manager = ExperimentManager()
        self.model_registry = ModelRegistry()
        
        # å½“å‰å®éªŒ
        self.current_experiment_id = None
        self.metrics_tracker = None
        
        # è®­ç»ƒçŠ¶æ€
        self.training_started = False
        self.training_completed = False
        
    def create_experiment(self, experiment_name: str, description: str, 
                         model_name: str, dataset_name: str, 
                         hyperparameters: Dict[str, Any]) -> str:
        """åˆ›å»ºæ–°å®éªŒ"""
        try:
            # åˆ›å»ºå®éªŒé…ç½®
            config = ExperimentConfig(
                name=experiment_name,
                description=description,
                model_name=model_name,
                dataset_name=dataset_name,
                hyperparameters=hyperparameters,
                training_config=self.config_manager.get_training_config(),
                created_at=datetime.now().isoformat(),
                created_by="production_trainer",
                tags=["production", "eit-p"]
            )
            
            # åˆ›å»ºå®éªŒ
            experiment_id = self.experiment_manager.create_experiment(config)
            self.current_experiment_id = experiment_id
            
            # åˆå§‹åŒ–æŒ‡æ ‡è·Ÿè¸ªå™¨
            self.metrics_tracker = MetricsTracker(experiment_id)
            self.metrics_tracker.start_monitoring()
            
            self.logger.info(f"åˆ›å»ºå®éªŒ: {experiment_id} - {experiment_name}")
            return experiment_id
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå®éªŒå¤±è´¥: {e}")
            raise
    
    def setup_model_and_data(self, model_name: str, dataset_path: str):
        """è®¾ç½®æ¨¡å‹å’Œæ•°æ®"""
        try:
            self.logger.info("è®¾ç½®æ¨¡å‹å’Œæ•°æ®...")
            
            # åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                block_size=self.config_manager.get('model.block_size', 16),
                output_hidden_states=True,
                low_cpu_mem_usage=True,
                torch_dtype=torch.float32
            )
            
            # åŠ è½½åˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½æ•°æ®é›†
            train_dataset = TextDataset(
                tokenizer=tokenizer,
                file_path=dataset_path,
                block_size=self.config_manager.get('model.block_size', 16)
            )
            
            # æ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False
            )
            
            self.logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
            self.logger.info(f"è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset):,}")
            
            return model, tokenizer, train_dataset, data_collator
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®æ¨¡å‹å’Œæ•°æ®å¤±è´¥: {e}")
            raise
    
    def train(self, experiment_id: str, model: torch.nn.Module, 
              train_dataset: TextDataset, data_collator: DataCollatorForLanguageModeling,
              hypernetwork_params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒ"""
        try:
            self.logger.info(f"å¼€å§‹è®­ç»ƒå®éªŒ: {experiment_id}")
            
            # å¼€å§‹å®éªŒ
            self.experiment_manager.start_experiment(experiment_id)
            self.training_started = True
            
            # è®¾ç½®è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=f"./experiments/experiments/{experiment_id}",
                **self.config_manager.get_training_config()
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = EITPTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=None,
                data_collator=data_collator,
                hypernetwork_params=hypernetwork_params,
                config_manager=self.config_manager
            )
            
            # æ·»åŠ æŒ‡æ ‡è·Ÿè¸ªå›è°ƒ
            trainer.add_callback(MetricsCallback(self.metrics_tracker))
            
            # å¼€å§‹è®­ç»ƒ
            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time
            
            # è®­ç»ƒå®Œæˆ
            self.training_completed = True
            
            # æ”¶é›†æœ€ç»ˆç»“æœ
            results = {
                'training_time': training_time,
                'final_loss': trainer.state.log_history[-1].get('train_loss', 0),
                'total_steps': trainer.state.global_step,
                'model_parameters': sum(p.numel() for p in model.parameters()),
                'completed_at': datetime.now().isoformat()
            }
            
            # å®Œæˆå®éªŒ
            self.experiment_manager.complete_experiment(experiment_id, results)
            
            # æ³¨å†Œæ¨¡å‹
            self.register_model(experiment_id, model, results)
            
            self.logger.info(f"è®­ç»ƒå®Œæˆ: {experiment_id}")
            return results
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            
            # æ ‡è®°å®éªŒå¤±è´¥
            if self.current_experiment_id:
                self.experiment_manager.fail_experiment(experiment_id, str(e))
            
            raise
    
    def register_model(self, experiment_id: str, model: torch.nn.Module, 
                      results: Dict[str, Any]) -> str:
        """æ³¨å†Œæ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨¡å‹å…ƒæ•°æ®
            model_id = f"eitp_{experiment_id}_{int(time.time())}"
            
            metadata = ModelMetadata(
                model_id=model_id,
                name=f"EIT-P Model {experiment_id}",
                version="1.0.0",
                description=f"EIT-Pæ¨¡å‹ - å®éªŒ {experiment_id}",
                model_type="eit-p",
                architecture="gpt2",
                parameters=sum(p.numel() for p in model.parameters()),
                size_mb=sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2,
                created_at=datetime.now().isoformat(),
                created_by="production_trainer",
                experiment_id=experiment_id,
                performance_metrics={
                    'final_loss': results.get('final_loss', 0),
                    'training_time': results.get('training_time', 0)
                },
                tags=["eit-p", "production"]
            )
            
            # æ³¨å†Œæ¨¡å‹
            model_id = self.model_registry.register_model(model, metadata, experiment_id)
            
            self.logger.info(f"æ³¨å†Œæ¨¡å‹: {model_id}")
            return model_id
            
        except Exception as e:
            self.logger.error(f"æ³¨å†Œæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def run_training_pipeline(self, experiment_name: str, description: str,
                            model_name: str, dataset_path: str,
                            hyperparameters: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨EIT-Pç”Ÿäº§çº§è®­ç»ƒæµæ°´çº¿")
            
            # 1. åˆ›å»ºå®éªŒ
            experiment_id = self.create_experiment(
                experiment_name, description, model_name, 
                "custom_dataset", hyperparameters
            )
            
            # 2. è®¾ç½®æ¨¡å‹å’Œæ•°æ®
            model, tokenizer, train_dataset, data_collator = self.setup_model_and_data(
                model_name, dataset_path
            )
            
            # 3. è®¾ç½®è¶…ç½‘ç»œå‚æ•°
            hypernetwork_params = self.config_manager.get_hypernetwork_config()
            
            # 4. æ‰§è¡Œè®­ç»ƒ
            results = self.train(
                experiment_id, model, train_dataset, 
                data_collator, hypernetwork_params
            )
            
            # 5. è¿”å›ç»“æœ
            return {
                'experiment_id': experiment_id,
                'status': 'success',
                'results': results,
                'message': 'è®­ç»ƒå®Œæˆ'
            }
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
            return {
                'experiment_id': self.current_experiment_id,
                'status': 'failed',
                'error': str(e),
                'message': 'è®­ç»ƒå¤±è´¥'
            }
        
        finally:
            # æ¸…ç†èµ„æº
            if self.metrics_tracker:
                self.metrics_tracker.stop_monitoring()


class MetricsCallback:
    """æŒ‡æ ‡å›è°ƒ"""
    
    def __init__(self, metrics_tracker: MetricsTracker):
        self.metrics_tracker = metrics_tracker
    
    def on_log(self, args, state, control, model=None, logs=None, **kwargs):
        """è®­ç»ƒæ—¥å¿—å›è°ƒ"""
        if logs:
            # è®°å½•æŒ‡æ ‡
            self.metrics_tracker.log_metrics_batch(
                logs, 
                step=state.global_step,
                epoch=state.epoch
            )
            
            # è®¾ç½®å½“å‰æ­¥æ•°å’Œè½®æ¬¡
            self.metrics_tracker.set_current_step(state.global_step)
            self.metrics_tracker.set_current_epoch(int(state.epoch))


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ EIT-P ç”Ÿäº§çº§è®­ç»ƒè„šæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 4:
        print("ç”¨æ³•: python production_train.py <experiment_name> <model_name> <dataset_path>")
        print("ç¤ºä¾‹: python production_train.py 'my_experiment' 'gpt2' './data/train.txt'")
        sys.exit(1)
    
    experiment_name = sys.argv[1]
    model_name = sys.argv[2]
    dataset_path = sys.argv[3]
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    if not Path(dataset_path).exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ProductionTrainer()
    
    # è®¾ç½®è¶…å‚æ•°
    hyperparameters = {
        'learning_rate': 5e-5,
        'batch_size': 1,
        'gradient_accumulation_steps': 16,
        'max_grad_norm': 0.1,
        'warmup_steps': 10,
        'num_epochs': 1
    }
    
    # è¿è¡Œè®­ç»ƒ
    try:
        results = trainer.run_training_pipeline(
            experiment_name=experiment_name,
            description=f"EIT-På®éªŒ: {experiment_name}",
            model_name=model_name,
            dataset_path=dataset_path,
            hyperparameters=hyperparameters
        )
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("=" * 50)
        print(f"å®éªŒID: {results['experiment_id']}")
        print(f"çŠ¶æ€: {results['status']}")
        
        if results['status'] == 'success':
            print(f"æœ€ç»ˆæŸå¤±: {results['results']['final_loss']:.4f}")
            print(f"è®­ç»ƒæ—¶é—´: {results['results']['training_time']:.2f}ç§’")
            print(f"æ€»æ­¥æ•°: {results['results']['total_steps']}")
        else:
            print(f"é”™è¯¯: {results['error']}")
        
        print("\nğŸ“Š æŸ¥çœ‹ç»“æœ:")
        print(f"  å®éªŒè¯¦æƒ…: http://localhost:8083/api/experiments/{results['experiment_id']}")
        print(f"  ç›‘æ§ä»ªè¡¨æ¿: http://localhost:8082")
        print(f"  æ¨¡å‹åˆ—è¡¨: http://localhost:8083/api/models")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
