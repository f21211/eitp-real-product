#!/usr/bin/env python3
"""
Enhanced CEP-EIT-P Optimization Tools
ä¼˜åŒ–å·¥å…·ï¼šè¶…å‚æ•°è°ƒä¼˜ã€æ¨¡å‹å‹ç¼©ã€é‡åŒ–
"""

import sys
import os
sys.path.append('/mnt/sda1/myproject/datainall/AGI')

import torch
import numpy as np
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from eit_p_enhanced_cep import EnhancedCEPEITP, CEPParameters
from advanced_features_manager import AdvancedFeaturesManager

@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    method: str = "bayesian"  # bayesian, grid, random
    max_trials: int = 100
    timeout: int = 3600  # seconds
    target_metric: str = "consciousness_level"
    target_value: float = 3.0

@dataclass
class CompressionConfig:
    """å‹ç¼©é…ç½®"""
    method: str = "pruning"  # pruning, quantization, distillation
    target_ratio: float = 0.5  # å‹ç¼©æ¯”ä¾‹
    preserve_accuracy: bool = True

class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.setup_logging()
        self.trial_history = []
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("HyperparameterOptimizer")
    
    def bayesian_optimization(self, config: OptimizationConfig) -> Dict:
        """è´å¶æ–¯ä¼˜åŒ–"""
        self.logger.info("å¼€å§‹è´å¶æ–¯ä¼˜åŒ–")
        
        best_params = None
        best_score = -float('inf')
        
        for trial in range(config.max_trials):
            # ç”Ÿæˆå€™é€‰å‚æ•°
            candidate_params = self._generate_candidate_params()
            
            # è¯„ä¼°å‚æ•°
            score = self._evaluate_params(candidate_params, config)
            
            # è®°å½•è¯•éªŒ
            trial_result = {
                'trial': trial,
                'params': candidate_params,
                'score': score,
                'timestamp': datetime.now().isoformat()
            }
            self.trial_history.append(trial_result)
            
            # æ›´æ–°æœ€ä½³å‚æ•°
            if score > best_score:
                best_score = score
                best_params = candidate_params
                self.logger.info(f"è¯•éªŒ {trial}: æ–°æœ€ä½³åˆ†æ•° {score:.4f}")
            
            if trial % 10 == 0:
                self.logger.info(f"å®Œæˆ {trial}/{config.max_trials} è¯•éªŒ")
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'total_trials': len(self.trial_history),
            'optimization_method': 'bayesian'
        }
    
    def grid_search(self, config: OptimizationConfig) -> Dict:
        """ç½‘æ ¼æœç´¢"""
        self.logger.info("å¼€å§‹ç½‘æ ¼æœç´¢")
        
        # å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grid = {
            'fractal_dimension': [2.0, 2.5, 3.0, 3.5],
            'complexity_coefficient': [0.5, 0.7, 0.9, 1.0],
            'critical_temperature': [0.5, 1.0, 1.5, 2.0],
            'field_strength': [0.5, 1.0, 1.5, 2.0],
            'entropy_balance': [-0.5, 0.0, 0.5, 1.0]
        }
        
        best_params = None
        best_score = -float('inf')
        total_combinations = np.prod([len(v) for v in param_grid.values()])
        
        trial = 0
        for fd in param_grid['fractal_dimension']:
            for cc in param_grid['complexity_coefficient']:
                for ct in param_grid['critical_temperature']:
                    for fs in param_grid['field_strength']:
                        for eb in param_grid['entropy_balance']:
                            candidate_params = {
                                'fractal_dimension': fd,
                                'complexity_coefficient': cc,
                                'critical_temperature': ct,
                                'field_strength': fs,
                                'entropy_balance': eb
                            }
                            
                            score = self._evaluate_params(candidate_params, config)
                            
                            trial_result = {
                                'trial': trial,
                                'params': candidate_params,
                                'score': score,
                                'timestamp': datetime.now().isoformat()
                            }
                            self.trial_history.append(trial_result)
                            
                            if score > best_score:
                                best_score = score
                                best_params = candidate_params
                                self.logger.info(f"è¯•éªŒ {trial}: æ–°æœ€ä½³åˆ†æ•° {score:.4f}")
                            
                            trial += 1
                            
                            if trial % 50 == 0:
                                self.logger.info(f"å®Œæˆ {trial}/{total_combinations} è¯•éªŒ")
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'total_trials': len(self.trial_history),
            'optimization_method': 'grid_search'
        }
    
    def random_search(self, config: OptimizationConfig) -> Dict:
        """éšæœºæœç´¢"""
        self.logger.info("å¼€å§‹éšæœºæœç´¢")
        
        best_params = None
        best_score = -float('inf')
        
        for trial in range(config.max_trials):
            candidate_params = self._generate_candidate_params()
            score = self._evaluate_params(candidate_params, config)
            
            trial_result = {
                'trial': trial,
                'params': candidate_params,
                'score': score,
                'timestamp': datetime.now().isoformat()
            }
            self.trial_history.append(trial_result)
            
            if score > best_score:
                best_score = score
                best_params = candidate_params
                self.logger.info(f"è¯•éªŒ {trial}: æ–°æœ€ä½³åˆ†æ•° {score:.4f}")
            
            if trial % 20 == 0:
                self.logger.info(f"å®Œæˆ {trial}/{config.max_trials} è¯•éªŒ")
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'total_trials': len(self.trial_history),
            'optimization_method': 'random_search'
        }
    
    def _generate_candidate_params(self) -> Dict:
        """ç”Ÿæˆå€™é€‰å‚æ•°"""
        return {
            'fractal_dimension': np.random.uniform(2.0, 4.0),
            'complexity_coefficient': np.random.uniform(0.1, 1.0),
            'critical_temperature': np.random.uniform(0.1, 3.0),
            'field_strength': np.random.uniform(0.1, 3.0),
            'entropy_balance': np.random.uniform(-1.0, 1.0)
        }
    
    def _evaluate_params(self, params: Dict, config: OptimizationConfig) -> float:
        """è¯„ä¼°å‚æ•°"""
        try:
            # åˆ›å»ºæ¨¡å‹
            cep_params = CEPParameters(**params)
            model = EnhancedCEPEITP(
                input_dim=784,
                hidden_dims=[512, 256, 128],
                output_dim=10,
                cep_params=cep_params
            )
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_input = torch.randn(10, 784)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                output, metrics = model(test_input)
                consciousness_level = metrics['consciousness_metrics'].consciousness_level
            
            # è®¡ç®—åˆ†æ•°
            if config.target_metric == "consciousness_level":
                score = consciousness_level
            else:
                score = consciousness_level  # é»˜è®¤ä½¿ç”¨æ„è¯†æ°´å¹³
            
            return float(score)
            
        except Exception as e:
            self.logger.warning(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
            return 0.0

class ModelCompressor:
    """æ¨¡å‹å‹ç¼©å™¨"""
    
    def __init__(self):
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("ModelCompressor")
    
    def prune_model(self, model: EnhancedCEPEITP, config: CompressionConfig) -> EnhancedCEPEITP:
        """æ¨¡å‹å‰ªæ"""
        self.logger.info(f"å¼€å§‹æ¨¡å‹å‰ªæï¼Œç›®æ ‡å‹ç¼©æ¯”ä¾‹: {config.target_ratio}")
        
        # è®¡ç®—å‰ªæé˜ˆå€¼
        all_weights = []
        for name, param in model.named_parameters():
            if 'weight' in name and param.numel() > 0:
                all_weights.append(param.data.abs().view(-1))
        
        if not all_weights:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯å‰ªæçš„æƒé‡å‚æ•°")
            return model
        
        all_weights = torch.cat(all_weights)
        threshold = torch.quantile(all_weights, config.target_ratio)
        
        # åº”ç”¨å‰ªæ
        pruned_params = 0
        total_params = 0
        
        for name, param in model.named_parameters():
            if 'weight' in name:
                mask = param.data.abs() > threshold
                param.data *= mask.float()
                
                pruned_params += (mask == 0).sum().item()
                total_params += param.numel()
        
        compression_ratio = pruned_params / total_params
        self.logger.info(f"å‰ªæå®Œæˆï¼Œå‹ç¼©æ¯”ä¾‹: {compression_ratio:.3f}")
        
        return model
    
    def quantize_model(self, model: EnhancedCEPEITP, config: CompressionConfig) -> EnhancedCEPEITP:
        """æ¨¡å‹é‡åŒ–"""
        self.logger.info("å¼€å§‹æ¨¡å‹é‡åŒ–")
        
        # ç®€å•çš„æƒé‡é‡åŒ–
        for name, param in model.named_parameters():
            if 'weight' in name:
                # é‡åŒ–åˆ°8ä½
                param.data = torch.round(param.data * 127) / 127
        
        self.logger.info("é‡åŒ–å®Œæˆ")
        return model
    
    def compress_model(self, model: EnhancedCEPEITP, config: CompressionConfig) -> Dict:
        """å‹ç¼©æ¨¡å‹"""
        self.logger.info(f"å¼€å§‹æ¨¡å‹å‹ç¼©: {config.method}")
        
        original_size = self._calculate_model_size(model)
        
        if config.method == "pruning":
            compressed_model = self.prune_model(model, config)
        elif config.method == "quantization":
            compressed_model = self.quantize_model(model, config)
        else:
            self.logger.warning(f"æœªçŸ¥å‹ç¼©æ–¹æ³•: {config.method}")
            return {'error': f'æœªçŸ¥å‹ç¼©æ–¹æ³•: {config.method}'}
        
        compressed_size = self._calculate_model_size(compressed_model)
        compression_ratio = (original_size - compressed_size) / original_size
        
        return {
            'original_size': original_size,
            'compressed_size': compressed_size,
            'compression_ratio': compression_ratio,
            'method': config.method
        }
    
    def _calculate_model_size(self, model: EnhancedCEPEITP) -> int:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰"""
        total_params = 0
        for param in model.parameters():
            total_params += param.numel()
        return total_params

class OptimizationToolsManager:
    """ä¼˜åŒ–å·¥å…·ç®¡ç†å™¨"""
    
    def __init__(self):
        self.hyperparameter_optimizer = HyperparameterOptimizer()
        self.model_compressor = ModelCompressor()
        self.advanced_manager = AdvancedFeaturesManager()
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("OptimizationToolsManager")
    
    def optimize_hyperparameters(self, config: OptimizationConfig) -> Dict:
        """ä¼˜åŒ–è¶…å‚æ•°"""
        self.logger.info("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–")
        
        if config.method == "bayesian":
            result = self.hyperparameter_optimizer.bayesian_optimization(config)
        elif config.method == "grid":
            result = self.hyperparameter_optimizer.grid_search(config)
        elif config.method == "random":
            result = self.hyperparameter_optimizer.random_search(config)
        else:
            raise ValueError(f"æœªçŸ¥ä¼˜åŒ–æ–¹æ³•: {config.method}")
        
        return result
    
    def compress_model(self, model: EnhancedCEPEITP, config: CompressionConfig) -> Dict:
        """å‹ç¼©æ¨¡å‹"""
        return self.model_compressor.compress_model(model, config)
    
    def run_optimization_pipeline(self, optimization_config: OptimizationConfig, 
                                 compression_config: CompressionConfig) -> Dict:
        """è¿è¡Œä¼˜åŒ–æµæ°´çº¿"""
        self.logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–æµæ°´çº¿")
        
        pipeline_results = {
            'start_time': datetime.now().isoformat(),
            'stages': {}
        }
        
        try:
            # 1. è¶…å‚æ•°ä¼˜åŒ–
            self.logger.info("ğŸ“Š é˜¶æ®µ1: è¶…å‚æ•°ä¼˜åŒ–")
            hyperopt_result = self.optimize_hyperparameters(optimization_config)
            pipeline_results['stages']['hyperparameter_optimization'] = hyperopt_result
            
            # 2. ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°åˆ›å»ºæ¨¡å‹
            self.logger.info("ğŸ—ï¸ é˜¶æ®µ2: åˆ›å»ºä¼˜åŒ–æ¨¡å‹")
            best_params = hyperopt_result['best_params']
            cep_params = CEPParameters(**best_params)
            optimized_model = EnhancedCEPEITP(
                input_dim=784,
                hidden_dims=[512, 256, 128],
                output_dim=10,
                cep_params=cep_params
            )
            
            # 3. æ¨¡å‹å‹ç¼©
            self.logger.info("ğŸ—œï¸ é˜¶æ®µ3: æ¨¡å‹å‹ç¼©")
            compression_result = self.compress_model(optimized_model, compression_config)
            pipeline_results['stages']['model_compression'] = compression_result
            
            # 4. æ€§èƒ½è¯„ä¼°
            self.logger.info("ğŸ“ˆ é˜¶æ®µ4: æ€§èƒ½è¯„ä¼°")
            performance = self._evaluate_optimized_model(optimized_model)
            pipeline_results['stages']['performance_evaluation'] = performance
            
            pipeline_results['status'] = 'success'
            pipeline_results['end_time'] = datetime.now().isoformat()
            
            self.logger.info("ğŸ‰ ä¼˜åŒ–æµæ°´çº¿å®Œæˆ!")
            
        except Exception as e:
            pipeline_results['status'] = 'failed'
            pipeline_results['error'] = str(e)
            pipeline_results['end_time'] = datetime.now().isoformat()
            self.logger.error(f"âŒ ä¼˜åŒ–æµæ°´çº¿å¤±è´¥: {e}")
        
        return pipeline_results
    
    def _evaluate_optimized_model(self, model: EnhancedCEPEITP) -> Dict:
        """è¯„ä¼°ä¼˜åŒ–åçš„æ¨¡å‹"""
        model.eval()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_input = torch.randn(100, 784)
        
        with torch.no_grad():
            start_time = time.time()
            output, metrics = model(test_input)
            inference_time = time.time() - start_time
        
        return {
            'consciousness_level': metrics['consciousness_metrics'].consciousness_level,
            'inference_time': inference_time,
            'model_size': self.model_compressor._calculate_model_size(model),
            'memory_usage': torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Enhanced CEP-EIT-P Optimization Tools")
    print("=" * 50)
    
    # åˆ›å»ºä¼˜åŒ–å·¥å…·ç®¡ç†å™¨
    optimization_manager = OptimizationToolsManager()
    
    # åˆ›å»ºä¼˜åŒ–é…ç½®
    optimization_config = OptimizationConfig(
        method="random",  # ä½¿ç”¨éšæœºæœç´¢è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        max_trials=20,
        target_metric="consciousness_level",
        target_value=3.0
    )
    
    # åˆ›å»ºå‹ç¼©é…ç½®
    compression_config = CompressionConfig(
        method="pruning",
        target_ratio=0.3,
        preserve_accuracy=True
    )
    
    # è¿è¡Œä¼˜åŒ–æµæ°´çº¿
    results = optimization_manager.run_optimization_pipeline(
        optimization_config, 
        compression_config
    )
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š ä¼˜åŒ–ç»“æœ:")
    print(f"çŠ¶æ€: {results['status']}")
    
    if results['status'] == 'success':
        hyperopt = results['stages']['hyperparameter_optimization']
        compression = results['stages']['model_compression']
        performance = results['stages']['performance_evaluation']
        
        print(f"\nğŸ¯ è¶…å‚æ•°ä¼˜åŒ–:")
        print(f"  æœ€ä½³åˆ†æ•°: {hyperopt['best_score']:.4f}")
        print(f"  æ€»è¯•éªŒæ•°: {hyperopt['total_trials']}")
        print(f"  æœ€ä½³å‚æ•°: {hyperopt['best_params']}")
        
        print(f"\nğŸ—œï¸ æ¨¡å‹å‹ç¼©:")
        print(f"  åŸå§‹å¤§å°: {compression['original_size']}")
        print(f"  å‹ç¼©åå¤§å°: {compression['compressed_size']}")
        print(f"  å‹ç¼©æ¯”ä¾‹: {compression['compression_ratio']:.3f}")
        
        print(f"\nğŸ“ˆ æ€§èƒ½è¯„ä¼°:")
        print(f"  æ„è¯†æ°´å¹³: {performance['consciousness_level']:.3f}")
        print(f"  æ¨ç†æ—¶é—´: {performance['inference_time']:.4f}s")
        print(f"  æ¨¡å‹å¤§å°: {performance['model_size']}")
        print(f"  å†…å­˜ä½¿ç”¨: {performance['memory_usage']:.2f}MB")
    
    print("ğŸ‰ ä¼˜åŒ–å·¥å…·æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()