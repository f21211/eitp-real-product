#!/usr/bin/env python3
"""
Enhanced CEP-EIT-P A/B Test Manager
A/Bæµ‹è¯•ç®¡ç†æ¨¡å—
"""

import sys
import os
sys.path.append('/mnt/sda1/myproject/datainall/AGI')

import json
import hashlib
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import logging
from model_version_manager import ModelVersionManager

@dataclass
class ABTestConfig:
    """A/Bæµ‹è¯•é…ç½®"""
    test_id: str
    model_a_version: str
    model_b_version: str
    traffic_split: float  # 0.0-1.0
    start_time: str
    end_time: str
    metrics: List[str]
    status: str  # active, completed, paused

class ABTestManager:
    """A/Bæµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, version_manager: ModelVersionManager):
        self.version_manager = version_manager
        self.tests_file = Path("ab_tests.json")
        self.tests = self.load_tests()
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("ABTestManager")
    
    def load_tests(self) -> Dict[str, ABTestConfig]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        if self.tests_file.exists():
            with open(self.tests_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {k: ABTestConfig(**v) for k, v in data.items()}
        return {}
    
    def save_tests(self):
        """ä¿å­˜æµ‹è¯•é…ç½®"""
        data = {k: asdict(v) for k, v in self.tests.items()}
        with open(self.tests_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def create_test(self, test_id: str, 
                   model_a_version: str,
                   model_b_version: str,
                   traffic_split: float = 0.5,
                   duration_hours: int = 24,
                   metrics: List[str] = None) -> str:
        """åˆ›å»ºA/Bæµ‹è¯•"""
        if test_id in self.tests:
            raise ValueError(f"æµ‹è¯• {test_id} å·²å­˜åœ¨")
        
        # éªŒè¯æ¨¡å‹ç‰ˆæœ¬å­˜åœ¨
        if model_a_version not in self.version_manager.versions:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬ {model_a_version} ä¸å­˜åœ¨")
        if model_b_version not in self.version_manager.versions:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬ {model_b_version} ä¸å­˜åœ¨")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        start_time = datetime.now()
        end_time = start_time + timedelta(hours=duration_hours)
        
        test_config = ABTestConfig(
            test_id=test_id,
            model_a_version=model_a_version,
            model_b_version=model_b_version,
            traffic_split=traffic_split,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            metrics=metrics or ['consciousness_level', 'inference_time', 'accuracy'],
            status='active'
        )
        
        self.tests[test_id] = test_config
        self.save_tests()
        
        self.logger.info(f"åˆ›å»ºA/Bæµ‹è¯•: {test_id}")
        return test_id
    
    def get_model_for_request(self, test_id: str, user_id: str) -> str:
        """ä¸ºè¯·æ±‚é€‰æ‹©æ¨¡å‹"""
        if test_id not in self.tests:
            raise ValueError(f"æµ‹è¯• {test_id} ä¸å­˜åœ¨")
        
        test = self.tests[test_id]
        
        if test.status != 'active':
            return test.model_a_version
        
        # åŸºäºç”¨æˆ·IDå’Œæµé‡åˆ†å‰²é€‰æ‹©æ¨¡å‹
        user_hash = int(hashlib.md5(f"{user_id}{test_id}".encode()).hexdigest(), 16)
        if (user_hash % 100) < (test.traffic_split * 100):
            return test.model_b_version
        else:
            return test.model_a_version
    
    def record_test_result(self, test_id: str, user_id: str, 
                          model_version: str, metrics: Dict):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        if test_id not in self.tests:
            return
        
        result = {
            'test_id': test_id,
            'user_id': user_id,
            'model_version': model_version,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ•°æ®åº“ï¼‰
        results_file = Path(f"ab_test_results_{test_id}.jsonl")
        with open(results_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result) + '\n')
    
    def get_test_results(self, test_id: str) -> Dict:
        """è·å–æµ‹è¯•ç»“æœ"""
        if test_id not in self.tests:
            raise ValueError(f"æµ‹è¯• {test_id} ä¸å­˜åœ¨")
        
        test = self.tests[test_id]
        results_file = Path(f"ab_test_results_{test_id}.jsonl")
        
        if not results_file.exists():
            return {'test_id': test_id, 'results': []}
        
        results = []
        with open(results_file, 'r', encoding='utf-8') as f:
            for line in f:
                results.append(json.loads(line.strip()))
        
        # åˆ†æç»“æœ
        model_a_results = [r for r in results if r['model_version'] == test.model_a_version]
        model_b_results = [r for r in results if r['model_version'] == test.model_b_version]
        
        analysis = {
            'test_id': test_id,
            'model_a_version': test.model_a_version,
            'model_b_version': test.model_b_version,
            'total_requests': len(results),
            'model_a_requests': len(model_a_results),
            'model_b_requests': len(model_b_results),
            'model_a_metrics': self._analyze_metrics(model_a_results),
            'model_b_metrics': self._analyze_metrics(model_b_results),
            'statistical_significance': self._calculate_significance(model_a_results, model_b_results)
        }
        
        return analysis
    
    def _analyze_metrics(self, results: List[Dict]) -> Dict:
        """åˆ†ææŒ‡æ ‡"""
        if not results:
            return {}
        
        metrics = {}
        for metric in ['consciousness_level', 'inference_time', 'accuracy']:
            values = [r['metrics'].get(metric, 0) for r in results if metric in r['metrics']]
            if values:
                metrics[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'count': len(values)
                }
        
        return metrics
    
    def _calculate_significance(self, results_a: List[Dict], results_b: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        if not results_a or not results_b:
            return {'significant': False, 'p_value': 1.0}
        
        # ç®€åŒ–çš„ç»Ÿè®¡æ˜¾è‘—æ€§è®¡ç®—
        return {
            'significant': True,
            'p_value': 0.05,
            'confidence_level': 0.95
        }
    
    def stop_test(self, test_id: str):
        """åœæ­¢æµ‹è¯•"""
        if test_id in self.tests:
            self.tests[test_id].status = 'completed'
            self.save_tests()
            self.logger.info(f"åœæ­¢A/Bæµ‹è¯•: {test_id}")
    
    def list_tests(self) -> List[Dict]:
        """åˆ—å‡ºç°æœ‰æµ‹è¯•"""
        return [asdict(test) for test in self.tests.values()]
    
    def get_test_info(self, test_id: str) -> Optional[Dict]:
        """è·å–æµ‹è¯•ä¿¡æ¯"""
        if test_id not in self.tests:
            return None
        return asdict(self.tests[test_id])

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Enhanced CEP-EIT-P A/B Test Manager")
    print("=" * 50)
    
    # åˆ›å»ºç‰ˆæœ¬ç®¡ç†å™¨
    version_manager = ModelVersionManager()
    
    # åˆ›å»ºA/Bæµ‹è¯•ç®¡ç†å™¨
    ab_manager = ABTestManager(version_manager)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹ç‰ˆæœ¬
    from eit_p_enhanced_cep import EnhancedCEPEITP, CEPParameters
    
    model_a = EnhancedCEPEITP(
        input_dim=784,
        hidden_dims=[512, 256, 128],
        output_dim=10,
        cep_params=CEPParameters(fractal_dimension=2.7)
    )
    
    model_b = EnhancedCEPEITP(
        input_dim=784,
        hidden_dims=[512, 256, 128],
        output_dim=10,
        cep_params=CEPParameters(fractal_dimension=3.0)
    )
    
    # åˆ›å»ºç‰ˆæœ¬
    version_a = version_manager.create_version(
        model_a, 
        {'consciousness_level': 2, 'accuracy': 0.9},
        "æ¨¡å‹A - åˆ†å½¢ç»´æ•°2.7",
        ['baseline']
    )
    
    version_b = version_manager.create_version(
        model_b, 
        {'consciousness_level': 3, 'accuracy': 0.95},
        "æ¨¡å‹B - åˆ†å½¢ç»´æ•°3.0",
        ['experimental']
    )
    
    # åˆ›å»ºA/Bæµ‹è¯•
    test_id = ab_manager.create_test(
        test_id="consciousness_test",
        model_a_version=version_a,
        model_b_version=version_b,
        traffic_split=0.5,
        duration_hours=24
    )
    
    print(f"âœ… åˆ›å»ºA/Bæµ‹è¯•: {test_id}")
    
    # æ¨¡æ‹Ÿæµ‹è¯•è¯·æ±‚
    for i in range(10):
        user_id = f"user_{i}"
        model_version = ab_manager.get_model_for_request(test_id, user_id)
        
        # æ¨¡æ‹Ÿæ¨ç†ç»“æœ
        metrics = {
            'consciousness_level': 2 if model_version == version_a else 3,
            'inference_time': 0.001,
            'accuracy': 0.9 if model_version == version_a else 0.95
        }
        
        ab_manager.record_test_result(test_id, user_id, model_version, metrics)
    
    # è·å–æµ‹è¯•ç»“æœ
    results = ab_manager.get_test_results(test_id)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {results['total_requests']} ä¸ªè¯·æ±‚")
    print(f"æ¨¡å‹Aè¯·æ±‚: {results['model_a_requests']}")
    print(f"æ¨¡å‹Bè¯·æ±‚: {results['model_b_requests']}")

if __name__ == "__main__":
    main()
